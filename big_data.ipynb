{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59491103aab1c9f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName('BigData').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2685b2fc36a86adf",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6bab4-3139-4bdc-a6c0-98ac2dc129ea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Đọc dữ liệu từ file csv\n",
    "train_df = spark.read.csv('bigmart-sales-data/Train.csv', header=True, inferSchema=True)\n",
    "test_df = spark.read.csv('bigmart-sales-data/Test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc312375aad63b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kiểm tra cấu trúc của dữ liệu\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc04c56f6027d4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Liệt kế giá trị null của từng đặc trưng\n",
    "missing_value = train_df.select(\n",
    "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_df.columns]).toPandas().T\n",
    "missing_value = missing_value.rename(columns={0: 'count'})\n",
    "print(missing_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395e33750b8bd68",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83937a8738dc764",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tính giá trị xuất hiện nhiều nhất trong tập dữ liệu\n",
    "mode_train = train_df.groupBy('Outlet_Size').count().orderBy('count', ascending=False).first()[0]\n",
    "\n",
    "# Thay thế giá trị null bằng giá trị xuất hiện nhiều nhất cho tập train\n",
    "train_df = train_df.fillna({'Outlet_Size': mode_train})\n",
    "\n",
    "# Kiểm tra lại giá trị null\n",
    "mode_test = test_df.groupBy('Outlet_Size').count().orderBy('count', ascending=False).first()[0]\n",
    "\n",
    "# Thay thế giá trị null bằng giá trị xuất hiện nhiều nhất cho tập test \n",
    "test_df = test_df.fillna({'Outlet_Size': mode_test})\n",
    "\n",
    "# Kiểm tra số lượng giá trị null còn lại của 'Outlet_Size'\n",
    "missing_train = train_df.filter(col('Outlet_Size').isNull()).count()\n",
    "missing_test = test_df.filter(col('Outlet_Size').isNull()).count()\n",
    "\n",
    "print(missing_train, missing_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ac79bedafbed8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Tính giá trị trung bình của 'Item_Weight'\n",
    "mean_train = train_df.select(mean(col('Item_Weight'))).collect()[0][0]\n",
    "\n",
    "mean_test = test_df.select(mean(col('Item_Weight'))).collect()[0][0]\n",
    "\n",
    "\n",
    "# Thay thế giá trị null bằng giá trị trung bình\n",
    "train_df = train_df.fillna({'Item_Weight': mean_train})\n",
    "\n",
    "test_df = test_df.fillna({'Item_Weight': mean_test})\n",
    "\n",
    "# Kiểm tra số lượng giá trị null còn lại của 'Item_Weight'\n",
    "missing_train = train_df.filter(col('Item_Weight').isNull()).count()\n",
    "missing_test = test_df.filter(col('Item_Weight').isNull()).count()\n",
    "\n",
    "print(missing_train, missing_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc40c91a7a59af06",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the column names and their data types\n",
    "column_data_types = train_df.dtypes\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "num_columns = [column for column, dtype in column_data_types if dtype in ['int', 'double']]\n",
    "cat_columns = [column for column, dtype in column_data_types if dtype == 'string']\n",
    "\n",
    "# Create dataframes for numeric and categorical columns\n",
    "BM_num = train_df.select(num_columns)\n",
    "BM_cat = train_df.select(cat_columns)\n",
    "\n",
    "# Print the column names\n",
    "print(num_columns)\n",
    "print(cat_columns)\n",
    "\n",
    "# Print the value counts for each categorical column\n",
    "for column in cat_columns[1:]:\n",
    "    train_df.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3d6eaa3534584",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fdf70d122166",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Đổi tên giá trị 'LF', 'low fat' thành 'Low Fat' và 'reg' thành 'Regular'\n",
    "train_df = train_df.withColumn('Item_Fat_Content',\n",
    "                               when(col('Item_Fat_Content') == 'LF', 'Low Fat')\n",
    "                               .when(col('Item_Fat_Content') == 'low fat', 'Low Fat')\n",
    "                               .when(col('Item_Fat_Content') == 'reg', 'Regular')\n",
    "                               .otherwise(col('Item_Fat_Content')))\n",
    "\n",
    "test_df = test_df.withColumn('Item_Fat_Content',\n",
    "                             when(col('Item_Fat_Content') == 'LF', 'Low Fat')\n",
    "                             .when(col('Item_Fat_Content') == 'low fat', 'Low Fat')\n",
    "                             .when(col('Item_Fat_Content') == 'reg', 'Regular')\n",
    "                             .otherwise(col('Item_Fat_Content')))\n",
    "\n",
    "# Kiểm tra lại giá trị của 'Item_Fat_Content'\n",
    "train_df.groupBy('Item_Fat_Content').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a56de996ec59b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from datetime import date\n",
    "\n",
    "# Tạo cột 'Outlet_Age' với giá trị là hiện tại trừ đi năm thành lập cửa hàng\n",
    "train_df = train_df.withColumn('Outlet_Age', lit(date.today().year) - col('Outlet_Establishment_Year'))\n",
    "\n",
    "test_df = test_df.withColumn('Outlet_Age', lit(date.today().year) - col('Outlet_Establishment_Year'))\n",
    "\n",
    "# Kiểm tra lại giá trị của 'Outlet_Age'\n",
    "train_df.select('Outlet_Age').show()\n",
    "test_df.select('Outlet_Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00f3c37394b238",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Tính số lượng giá trị duy nhất của từng cột\n",
    "unique_values = {column: BM_cat.select(countDistinct(column)).first()[0] for column in BM_cat.columns}\n",
    "\n",
    "# In ra số lượng giá trị duy nhất của từng cột\n",
    "for column, unique_count in unique_values.items():\n",
    "    print(f\"{column}: {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192770c0987f5052",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Liệt kê các cột cần được mã hóa (do có giá trị là string, chuỗi)\n",
    "columns_to_encode = ['Item_Fat_Content', 'Outlet_Size', 'Outlet_Location_Type']\n",
    "\n",
    "# Áp dụng StringIndexer (biến đổi chuỗi thành số) cho từng cột trong list\n",
    "for column in columns_to_encode:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\")\n",
    "    train_df = indexer.fit(train_df).transform(train_df)\n",
    "    test_df = indexer.fit(test_df).transform(test_df)\n",
    "\n",
    "# Hiển thị dữ liệu\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3bb3739c5b83a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Liệt kê các cột cần được mã hóa (do có giá trị là string, chuỗi)\n",
    "columns_to_encode = ['Item_Type', 'Outlet_Type']\n",
    "\n",
    "# Áp dụng StringIndexer (biến đổi chuỗi thành số) và OneHotEncoder (biến đổi số thành vector) cho từng cột trong danh sách cột cần được mã hóa\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    # StringIndexer\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\")\n",
    "    train_indexed = indexer.fit(train_df).transform(train_df)\n",
    "    test_indexed = indexer.fit(test_df).transform(test_df)\n",
    "\n",
    "    # OneHotEncoder\n",
    "    encoder = OneHotEncoder(inputCols=[column + \"_index\"], outputCols=[column + \"_vec\"])\n",
    "    train_fe = encoder.fit(train_indexed).transform(train_indexed)\n",
    "    test_fe = encoder.fit(test_indexed).transform(test_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c65834baa522b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Danh sách các cột sẽ bị xóa\n",
    "columns_to_drop = ['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Type', 'Item_Type',\n",
    "                   'Item_Fat_Content', 'Outlet_Size', 'Outlet_Location_Type']\n",
    "\n",
    "# Xoá các cột trong danh sách ở trên\n",
    "train_fe = train_df.drop(*columns_to_drop)\n",
    "\n",
    "test_fe = test_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1a41bac649158",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Danh sách các cột đầu vào\n",
    "input_columns = [column for column in train_fe.columns if column != 'Item_Outlet_Sales']\n",
    "\n",
    "# Khỏw tạo đối tượng VectorAssembler (tạo vector đầu vào)\n",
    "assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "\n",
    "# Áp dụng VectorAssembler cho tập train và test\n",
    "train_data = assembler.transform(train_fe)\n",
    "test_data = assembler.transform(test_fe)\n",
    "\n",
    "# Now 'features' column is added to the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e56114c97ecbf7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chia tập dữ liệu thành tập train và test\n",
    "X_train, X_test = train_data.randomSplit([0.8, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128ba18bd8a3afc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "def cross_val(model_name, model, X, y, num_folds):\n",
    "    \"\"\"\n",
    "    :param model_name: Tên của mô hình \n",
    "    :param model: Mô hình\n",
    "    :param X: Tập dữ liệu đầu vào\n",
    "    :param y: Tên cột chứa giá trị mục tiêu\n",
    "    :param num_folds: Số lượng folds (folds: tập con)\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    # tạo lưới tham số (lưới tham số: tập các tham số mà mô hình sẽ thử)\n",
    "    paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "    # Khởi tạo đối tượng RegressionEvaluator (đánh giá mô hình hồi quy)\n",
    "    evaluator = RegressionEvaluator(labelCol=y, predictionCol=\"prediction\")\n",
    "\n",
    "    # Khởi tạo đối tượng CrossValidator (xác thực chéo)\n",
    "    crossval = CrossValidator(estimator=model,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=num_folds)\n",
    "\n",
    "    # Fit the model\n",
    "    cv_model = crossval.fit(X)\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = cv_model.transform(X)\n",
    "\n",
    "    # Calculate R2 score\n",
    "    r2_score = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "    print(f\"{model_name} - R2 score: {r2_score:.3f}\")\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "    print(f\"{model_name} - RMSE: {rmse:.3f}\")\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "    print(f\"{model_name} - MSE: {mse:.3f}\")\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "    print(f\"{model_name} - MAE: {mae:.3f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f9c7f797531fb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='Item_Outlet_Sales', maxIter=10, regParam=0.3,\n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model = lr.fit(X_train)  # Use the complete training data with features and target variable\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = lr_model.transform(X_test)\n",
    "\n",
    "# Cross validate the Linear Regression model\n",
    "cross_val('Linear Regression', lr, train_data, 'Item_Outlet_Sales', 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4957402759cac5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Thêm côt 'residuals' (sai số) cho tập dữ liệu\n",
    "predictions = predictions.withColumn('residuals', col('Item_Outlet_Sales') - col('prediction'))\n",
    "\n",
    "actual_pred_df = predictions.select(['Item_Outlet_Sales', 'prediction', 'residuals'])\n",
    "\n",
    "actual_pred_df.show()\n",
    "\n",
    "actual_pred_df.write.csv('predictions.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
