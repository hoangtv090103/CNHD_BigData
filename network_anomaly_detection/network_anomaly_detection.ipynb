{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T03:56:05.636656Z",
     "start_time": "2023-11-11T03:56:05.610095Z"
    }
   },
   "id": "bc61b6e96eb4853a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\",\n",
    "    \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\",\n",
    "    \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\",\n",
    "    \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\",\n",
    "    \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\",\n",
    "    \"dst_host_srv_count\", \"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"attack\", \"last_flag\"]\n",
    "\n",
    "print(len(features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T03:56:05.636954Z",
     "start_time": "2023-11-11T03:56:05.617362Z"
    }
   },
   "id": "a27d980fb93b86aa"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/hoangtv/PycharmProjects/BigData/network_anomaly_detection/Train.txt.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTrain.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minferSchema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m test_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m train_df \u001B[38;5;241m=\u001B[39m train_df\u001B[38;5;241m.\u001B[39mtoDF(\u001B[38;5;241m*\u001B[39mfeatures)\n",
      "File \u001B[0;32m~/PycharmProjects/BigData/venv/lib/python3.11/site-packages/pyspark/sql/readwriter.py:740\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    739\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n",
      "File \u001B[0;32m~/PycharmProjects/BigData/venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/PycharmProjects/BigData/venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/hoangtv/PycharmProjects/BigData/network_anomaly_detection/Train.txt."
     ]
    }
   ],
   "source": [
    "train_df = spark.read.csv(\"Train.txt\", inferSchema=True)\n",
    "test_df = spark.read.csv(\"Test.txt\", inferSchema=True)\n",
    "train_df = train_df.toDF(*features)\n",
    "\n",
    "train_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T03:56:05.677950Z",
     "start_time": "2023-11-11T03:56:05.620306Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "# Get a list of all non-numeric columns\n",
    "non_numeric_cols = [field.name for field in train_df.schema.fields if not isinstance(field.dataType, NumericType)]\n",
    "\n",
    "# Select non-numeric columns from the DataFrame\n",
    "non_numeric_df = train_df.select(non_numeric_cols)\n",
    "\n",
    "non_numeric_df.show()\n",
    "\n",
    "# Count Distinct values in each non-numeric column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.660970Z"
    }
   },
   "id": "ddd32bc5263ab7bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "column_to_drop = ['land', 'urgent', 'numfailedlogins', 'numoutboundcmds']\n",
    "\n",
    "train_df.drop(*column_to_drop)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.662217Z"
    }
   },
   "id": "f0aa190724420e63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Check for null values and count them\n",
    "missing_value = train_df.select(\n",
    "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_df.columns]).toPandas().T\n",
    "missing_value = missing_value.rename(columns={0: 'count'})\n",
    "print(missing_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.663434Z"
    }
   },
   "id": "fe6d3317a99f9c10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "train_df = train_df.withColumn('attack', when(col('attack') != 'normal', 'attack').otherwise(col('attack')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.664579Z"
    }
   },
   "id": "522522c1333bd3a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# List of columns to be encoded\n",
    "columns_to_encode = ['protocol_type', 'service', 'flag', 'attack']\n",
    "\n",
    "# Apply StringIndexer to each column in the list\n",
    "for column in columns_to_encode:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\")\n",
    "    train_df = indexer.fit(train_df).transform(train_df)\n",
    "\n",
    "train_df = train_df.drop(*columns_to_encode)\n",
    "\n",
    "# Now each of the columns 'protocoltype', 'service', 'flag', 'attack' \n",
    "# has an additional corresponding column with suffix '_index' that contains its numerical representation."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.665656Z"
    }
   },
   "id": "862b5f00d216f766"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Danh sách các cột đầu vào\n",
    "input_columns = [column for column in train_df.columns if column != 'attack']\n",
    "\n",
    "# Khởi tạo đối tượng VectorAssembler (tạo vector đầu vào)\n",
    "assembler = VectorAssembler(inputCols=input_columns, outputCol='features')\n",
    "\n",
    "train_data = assembler.transform(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.666697Z"
    }
   },
   "id": "1bf39172ad2f76d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Assume that 'features' is the column containing the feature vectors\n",
    "# Create a StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(train_data)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "train_data = scalerModel.transform(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.667358Z"
    }
   },
   "id": "df58c3f83c990f17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Chia dữ liệu thành 2 tập train và test\n",
    "train_data, test_data = train_data.randomSplit([0.8, 0.2], seed=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.667992Z"
    }
   },
   "id": "ddbef74e88a65d20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Create a Linear Regression Estimator\n",
    "lr = LinearRegression(featuresCol='features', labelCol='attack_index')\n",
    "\n",
    "lr_model = lr.fit(train_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.668665Z"
    }
   },
   "id": "94751373c9f6e73c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "predictions.select('attack_index', 'prediction').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T03:56:05.669451Z"
    }
   },
   "id": "de98c7dfe82cd8bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
